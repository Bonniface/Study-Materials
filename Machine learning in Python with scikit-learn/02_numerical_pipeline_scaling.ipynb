{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d168d661",
   "metadata": {},
   "source": [
    "# Preprocessing for numerical features\n",
    "\n",
    "In this notebook, we will still use only numerical features.\n",
    "\n",
    "We will introduce these new aspects:\n",
    "\n",
    "* an example of preprocessing, namely **scaling numerical variables**;\n",
    "* using a scikit-learn **pipeline** to chain preprocessing and model\n",
    "  training.\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "First, let's load the full adult census dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97767ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adult_census = pd.read_csv(\"../datasets/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcf88bb",
   "metadata": {},
   "source": [
    "We will now drop the target from the data we will use to train our\n",
    "predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f0c8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = adult_census[target_name]\n",
    "data = adult_census.drop(columns=target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffed47",
   "metadata": {},
   "source": [
    "Then, we select only the numerical columns, as seen in the previous\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98dbc8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [\n",
    "    \"age\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "data_numeric = data[numerical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a668f3",
   "metadata": {},
   "source": [
    "Finally, we can divide our dataset into a train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049aad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data_numeric, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f17c03d",
   "metadata": {},
   "source": [
    "## Model fitting with preprocessing\n",
    "\n",
    "A range of preprocessing algorithms in scikit-learn allow us to transform\n",
    "the input data before training a model. In our case, we will standardize the\n",
    "data and then train a new logistic regression model on that new version of\n",
    "the dataset.\n",
    "\n",
    "Let's start by printing some statistics about the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276d8bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36631.000000</td>\n",
       "      <td>36631.000000</td>\n",
       "      <td>36631.000000</td>\n",
       "      <td>36631.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.642352</td>\n",
       "      <td>1087.077721</td>\n",
       "      <td>89.665311</td>\n",
       "      <td>40.431247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.725748</td>\n",
       "      <td>7522.692939</td>\n",
       "      <td>407.110175</td>\n",
       "      <td>12.423952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age  capital-gain  capital-loss  hours-per-week\n",
       "count  36631.000000  36631.000000  36631.000000    36631.000000\n",
       "mean      38.642352   1087.077721     89.665311       40.431247\n",
       "std       13.725748   7522.692939    407.110175       12.423952\n",
       "min       17.000000      0.000000      0.000000        1.000000\n",
       "25%       28.000000      0.000000      0.000000       40.000000\n",
       "50%       37.000000      0.000000      0.000000       40.000000\n",
       "75%       48.000000      0.000000      0.000000       45.000000\n",
       "max       90.000000  99999.000000   4356.000000       99.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33ba85",
   "metadata": {},
   "source": [
    "We see that the dataset's features span across different ranges. Some\n",
    "algorithms make some assumptions regarding the feature distributions and\n",
    "usually normalizing features will be helpful to address these assumptions.\n",
    "\n",
    "<div class=\"admonition tip alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Tip</p>\n",
    "<p>Here are some reasons for scaling features:</p>\n",
    "<ul class=\"last simple\">\n",
    "<li>Models that rely on the distance between a pair of samples, for instance\n",
    "k-nearest neighbors, should be trained on normalized features to make each\n",
    "feature contribute approximately equally to the distance computations.</li>\n",
    "<li>Many models such as logistic regression use a numerical solver (based on\n",
    "gradient descent) to find their optimal parameters. This solver converges\n",
    "faster when the features are scaled.</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Whether or not a machine learning model requires scaling the features depends\n",
    "on the model family. Linear models such as logistic regression generally\n",
    "benefit from scaling the features while other models such as decision trees\n",
    "do not need such preprocessing (but will not suffer from it).\n",
    "\n",
    "We show how to apply such normalization using a scikit-learn transformer\n",
    "called `StandardScaler`. This transformer shifts and scales each feature\n",
    "individually so that they all have a 0-mean and a unit standard deviation.\n",
    "\n",
    "We will investigate different steps used in scikit-learn to achieve such a\n",
    "transformation of the data.\n",
    "\n",
    "First, one needs to call the method `fit` in order to learn the scaling from\n",
    "the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64bcf449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d817a",
   "metadata": {},
   "source": [
    "The `fit` method for transformers is similar to the `fit` method for\n",
    "predictors. The main difference is that the former has a single argument (the\n",
    "data matrix), whereas the latter has two arguments (the data matrix and the\n",
    "target).\n",
    "\n",
    "![Transformer fit diagram](../figures/api_diagram-transformer.fit.svg)\n",
    "\n",
    "In this case, the algorithm needs to compute the mean and standard deviation\n",
    "for each feature and store them into some NumPy arrays. Here, these\n",
    "statistics are the model states.\n",
    "\n",
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">The fact that the model states of this scaler are arrays of means and\n",
    "standard deviations is specific to the <tt class=\"docutils literal\">StandardScaler</tt>. Other\n",
    "scikit-learn transformers will compute different statistics and store them\n",
    "as model states, in the same fashion.</p>\n",
    "</div>\n",
    "\n",
    "We can inspect the computed means and standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3489a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727bbc6",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">scikit-learn convention: if an attribute is learned from the data, its name\n",
    "ends with an underscore (i.e. <tt class=\"docutils literal\">_</tt>), as in <tt class=\"docutils literal\">mean_</tt> and <tt class=\"docutils literal\">scale_</tt> for the\n",
    "<tt class=\"docutils literal\">StandardScaler</tt>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b64ae5",
   "metadata": {},
   "source": [
    "Scaling the data is applied to each feature individually (i.e. each column in\n",
    "the data matrix). For each feature, we subtract its mean and divide by its\n",
    "standard deviation.\n",
    "\n",
    "Once we have called the `fit` method, we can perform data transformation by\n",
    "calling the method `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled = scaler.transform(data_train)\n",
    "data_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76b534",
   "metadata": {},
   "source": [
    "Let's illustrate the internal mechanism of the `transform` method and put it\n",
    "to perspective with what we already saw with predictors.\n",
    "\n",
    "![Transformer transform diagram](../figures/api_diagram-transformer.transform.svg)\n",
    "\n",
    "The `transform` method for transformers is similar to the `predict` method\n",
    "for predictors. It uses a predefined function, called a **transformation\n",
    "function**, and uses the model states and the input data. However, instead of\n",
    "outputting predictions, the job of the `transform` method is to output a\n",
    "transformed version of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a434d89",
   "metadata": {},
   "source": [
    "Finally, the method `fit_transform` is a shorthand method to call\n",
    "successively `fit` and then `transform`.\n",
    "\n",
    "![Transformer fit_transform diagram](../figures/api_diagram-transformer.fit_transform.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled = scaler.fit_transform(data_train)\n",
    "data_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb609ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled = pd.DataFrame(data_train_scaled,\n",
    "                                 columns=data_train.columns)\n",
    "data_train_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2879ed6",
   "metadata": {},
   "source": [
    "Notice that the mean of all the columns is close to 0 and the standard deviation\n",
    "in all cases is close to 1.\n",
    "We can also visualize the effect of `StandardScaler` using a jointplot to show\n",
    "both the histograms of the distributions and a scatterplot of any pair of numerical\n",
    "features at the same time. We can observe that `StandardScaler` does not change\n",
    "the structure of the data itself but the axes get shifted and scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b191a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot  as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# number of points to visualize to have a clearer plot\n",
    "num_points_to_plot = 300\n",
    "\n",
    "sns.jointplot(data=data_train[:num_points_to_plot], x=\"age\",\n",
    "              y=\"hours-per-week\", marginal_kws=dict(bins=15))\n",
    "plt.suptitle(\"Jointplot of 'age' vs 'hours-per-week' \\nbefore StandardScaler\", y=1.1)\n",
    "\n",
    "sns.jointplot(data=data_train_scaled[:num_points_to_plot], x=\"age\",\n",
    "              y=\"hours-per-week\", marginal_kws=dict(bins=15))\n",
    "_ = plt.suptitle(\"Jointplot of 'age' vs 'hours-per-week' \\nafter StandardScaler\", y=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ffa242",
   "metadata": {},
   "source": [
    "We can easily combine sequential operations with a scikit-learn\n",
    "`Pipeline`, which chains together operations and is used as any other\n",
    "classifier or regressor. The helper function `make_pipeline` will create a\n",
    "`Pipeline`: it takes as arguments the successive transformations to perform,\n",
    "followed by the classifier or regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc96ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23c8df",
   "metadata": {},
   "source": [
    "The `make_pipeline` function did not require us to give a name to each step.\n",
    "Indeed, it was automatically assigned based on the name of the classes\n",
    "provided; a `StandardScaler` will be a step named `\"standardscaler\"` in the\n",
    "resulting pipeline. We can check the name of each steps of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e23d128",
   "metadata": {},
   "source": [
    "This predictive pipeline exposes the same methods as the final predictor:\n",
    "`fit` and `predict` (and additionally `predict_proba`, `decision_function`,\n",
    "or `score`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb26337",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b58bf",
   "metadata": {},
   "source": [
    "We can represent the internal mechanism of a pipeline when calling `fit`\n",
    "by the following diagram:\n",
    "\n",
    "![pipeline fit diagram](../figures/api_diagram-pipeline.fit.svg)\n",
    "\n",
    "When calling `model.fit`, the method `fit_transform` from each underlying\n",
    "transformer (here a single transformer) in the pipeline will be called to:\n",
    "\n",
    "- learn their internal model states\n",
    "- transform the training data. Finally, the preprocessed data are provided to\n",
    "  train the predictor.\n",
    "\n",
    "To predict the targets given a test set, one uses the `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7320e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target = model.predict(data_test)\n",
    "predicted_target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a214929",
   "metadata": {},
   "source": [
    "Let's show the underlying mechanism:\n",
    "\n",
    "![pipeline predict diagram](../figures/api_diagram-pipeline.predict.svg)\n",
    "\n",
    "The method `transform` of each transformer (here a single transformer) is\n",
    "called to preprocess the data. Note that there is no need to call the `fit`\n",
    "method for these transformers because we are using the internal model states\n",
    "computed when calling `model.fit`. The preprocessed data is then provided to\n",
    "the predictor that will output the predicted target by calling its method\n",
    "`predict`.\n",
    "\n",
    "As a shorthand, we can check the score of the full predictive pipeline\n",
    "calling the method `model.score`. Thus, let's check the computational and\n",
    "generalization performance of such a predictive pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbe77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__class__.__name__\n",
    "score = model.score(data_test, target_test)\n",
    "print(f\"The accuracy using a {model_name} is {score:.3f} \"\n",
    "      f\"with a fitting time of {elapsed_time:.3f} seconds \"\n",
    "      f\"in {model[-1].n_iter_[0]} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae492cb",
   "metadata": {},
   "source": [
    "We could compare this predictive model with the predictive model used in\n",
    "the previous notebook which did not scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ecca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35041d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__class__.__name__\n",
    "score = model.score(data_test, target_test)\n",
    "print(f\"The accuracy using a {model_name} is {score:.3f} \"\n",
    "      f\"with a fitting time of {elapsed_time:.3f} seconds \"\n",
    "      f\"in {model.n_iter_[0]} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7228979",
   "metadata": {},
   "source": [
    "We see that scaling the data before training the logistic regression was\n",
    "beneficial in terms of computational performance. Indeed, the number of\n",
    "iterations decreased as well as the training time. The generalization\n",
    "performance did not change since both models converged.\n",
    "\n",
    "<div class=\"admonition warning alert alert-danger\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Warning</p>\n",
    "<p class=\"last\">Working with non-scaled data will potentially force the algorithm to iterate\n",
    "more as we showed in the example above. There is also the catastrophic\n",
    "scenario where the number of required iterations is larger than the maximum\n",
    "number of iterations allowed by the predictor (controlled by the <tt class=\"docutils literal\">max_iter</tt>)\n",
    "parameter. Therefore, before increasing <tt class=\"docutils literal\">max_iter</tt>, make sure that the data\n",
    "are well scaled.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4da47",
   "metadata": {},
   "source": [
    "In this notebook we:\n",
    "\n",
    "* saw the importance of **scaling numerical variables**;\n",
    "* used a **pipeline** to chain scaling and logistic regression training."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nbreset": "https://raw.githubusercontent.com/INRIA/scikit-learn-mooc/main/notebooks/02_numerical_pipeline_scaling.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
